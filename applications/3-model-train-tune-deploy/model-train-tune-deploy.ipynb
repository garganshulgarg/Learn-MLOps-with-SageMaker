{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b71290e-32c6-4aa8-b9e7-726aafd404e9",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc5612-861f-4738-b19a-34eec0723309",
   "metadata": {},
   "source": [
    "### Initialize AWS SageMaker Environment and Define Data Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bca9c-a47f-4639-a573-2a4700f9322e",
   "metadata": {},
   "source": [
    "This code sets up the environment for working with Amazon SageMaker by importing necessary libraries and initializing key variables. It begins by establishing an AWS SageMaker session and obtaining the execution role, which grants permissions to interact with AWS resources. A default S3 bucket is defined to store data, and a prefix is used to organize activity-specific data paths within this bucket. Paths for training, validation, and testing data are specified, each stored in its respective folder within the S3 bucket, making it easier to access and manage data for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9159f8e-b23a-487d-b172-aaccccba8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for SageMaker session, AWS SDK, and data manipulation\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Obtain the SageMaker execution role for the notebook, which grants permissions to access AWS resources\n",
    "role = get_execution_role()\n",
    "\n",
    "# Set the default S3 bucket for SageMaker sessions; if none exists, SageMaker will create one\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "# Define a prefix for S3 paths to organize activity-related data within the bucket\n",
    "prefix = 'mlops/activity-3'\n",
    "\n",
    "# Initiate a SageMaker session, which is used to handle interactions with SageMaker APIs\n",
    "sess = Session()\n",
    "\n",
    "# Define S3 paths for train, validation, and test data, organized by prefix within the S3 bucket\n",
    "train_path = f\"s3://{bucket}/{prefix}/train/\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation/\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd370c-8cea-434d-a293-068d82de4a4e",
   "metadata": {},
   "source": [
    "### Retrieve Amazon SageMaker XGBoost Container Image URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbdc9e-b246-46bc-b95f-9756fa4db731",
   "metadata": {},
   "source": [
    "This code retrieves the Amazon SageMaker container image URI for the latest version of the XGBoost framework, ensuring compatibility with the current AWS region. The container URI is essential for launching and managing an XGBoost training job on SageMaker, as it provides the runtime environment pre-configured with the necessary libraries and dependencies. This region-specific approach ensures that the container image URI corresponds to resources available in the current region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9d6572-1cf2-4bf7-9a3f-0a71304a86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Amazon SageMaker container URI for the latest version of the XGBoost framework\n",
    "# This is region-specific, so it uses the current AWS session's region\n",
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, \n",
    "                                          framework='xgboost', \n",
    "                                          version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56338f8-36b8-43d8-bbe5-1563fd485d76",
   "metadata": {},
   "source": [
    "### Define S3 Input Data Locations for SageMaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf052d-0e01-47b0-bcb5-9aecec27de8c",
   "metadata": {},
   "source": [
    "This code defines the input data locations for both training and validation datasets, stored in Amazon S3, to be used in an Amazon SageMaker training job. Using the TrainingInput class, SageMaker recognizes these data inputs and allows seamless access during training. The content_type parameter is set to 'csv' to indicate that the input files are in CSV format, which is essential for compatibility with SageMaker’s data ingestion pipeline. By organizing data in this way, training and validation data can be easily referenced and managed in subsequent stages of the machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d85016-754e-4c64-889a-b4f9799d7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 input locations for training and validation data\n",
    "# The `TrainingInput` class indicates that these data inputs will be used in a training job\n",
    "# `content_type='csv'` specifies that the data is in CSV format\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f's3://{bucket}/{prefix}/train',\n",
    "    content_type='csv'\n",
    ")\n",
    "\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f's3://{bucket}/{prefix}/validation/',\n",
    "    content_type='csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4b39c-f212-42e4-b1e6-6b3f4003c10d",
   "metadata": {},
   "source": [
    "### Set Up and Train an XGBoost Model on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2be42-ea9d-43b1-abbf-e060910fef58",
   "metadata": {},
   "source": [
    "This code configures and initiates the training of an XGBoost model on Amazon SageMaker. Using the Estimator class, it specifies key parameters such as the instance type, IAM role, and the S3 location where the trained model will be stored. The model's hyperparameters, including max_depth, eta, gamma, min_child_weight, and subsample, are set to optimize the XGBoost algorithm for binary classification. Finally, the .fit() method starts the training job using the designated training and validation datasets stored in S3, enabling SageMaker to process the data and train the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb5cb34-5635-4606-9cbd-d049acbbe39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: xgboost2024-11-12-13-21-34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgboost-2024-11-12-13-21-34-549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-12 13:21:38 Starting - Starting the training job...\n",
      "2024-11-12 13:21:53 Starting - Preparing the instances for training...\n",
      "2024-11-12 13:22:19 Downloading - Downloading input data...\n",
      "2024-11-12 13:22:49 Downloading - Downloading the training image......\n",
      "2024-11-12 13:24:07 Training - Training image download completed. Training in progress.\n",
      "2024-11-12 13:24:07 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2024-11-12:13:23:55:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2024-11-12:13:23:55:INFO] File size need to be processed in the node: 4.35mb. Available memory size in the node: 8461.23mb\u001b[0m\n",
      "\u001b[34m[2024-11-12:13:23:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[13:23:55] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[13:23:55] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2024-11-12:13:23:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[13:23:55] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[13:23:55] 8238x59 matrix with 486042 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[13:23:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.101384#011validation-error:0.102209\u001b[0m\n",
      "\u001b[34m[13:23:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.101037#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m[13:23:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.100205#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[13:23:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.100309#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[13:23:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.101141#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.100447#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[13:23:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.100864#011validation-error:0.100024\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.10076#011validation-error:0.100267\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.100482#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.100621#011validation-error:0.100267\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.100309#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.100066#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.099997#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.099997#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.099754#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.099719#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.099754#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.099684#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.099823#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.099858#011validation-error:0.101602\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.099927#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.099546#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.099268#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09906#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.098887#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.098921#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.099164#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.098991#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.099025#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.098783#011validation-error:0.100024\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.098644#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[13:23:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.098609#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 28 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.098262#011validation-error:0.099903\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.098297#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.098332#011validation-error:0.099903\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.098332#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.098019#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.098019#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.098019#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.098054#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.097985#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.098193#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.098089#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.098158#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.098262#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.098297#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.098193#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 20 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.098124#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 22 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.09795#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 34 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.098089#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.098124#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.097777#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[13:23:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.097569#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.097638#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.097777#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.097569#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.097673#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.097569#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.097465#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.097395#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.097395#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.097256#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.097256#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.097256#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.097187#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.09736#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.097291#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.09736#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 34 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.097326#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.097291#011validation-error:0.101602\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.097152#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.097083#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.097152#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.097014#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.097083#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 28 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.097048#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.09691#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.096771#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.096632#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.096806#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 30 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.096667#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 38 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.096632#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.096632#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.096632#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.096632#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.096736#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.096701#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.096597#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.096528#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.096493#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.096493#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.096493#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.096424#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[13:23:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.096355#011validation-error:0.100995\u001b[0m\n",
      "\n",
      "2024-11-12 13:24:21 Completed - Training job completed\n",
      "Training seconds: 121\n",
      "Billable seconds: 121\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "# Initialize a new SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "# Generate a unique model name based on the current time to ensure uniqueness\n",
    "model_name = \"xgboost\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + model_name)\n",
    "\n",
    "# Define an XGBoost estimator using the SageMaker Estimator API\n",
    "# `container`: URI of the XGBoost container image (retrieved earlier)\n",
    "# `role`: IAM role with permissions for SageMaker to access AWS resources\n",
    "# `instance_count`: Number of compute instances to use\n",
    "# `instance_type`: Type of SageMaker instance for training\n",
    "# `output_path`: S3 location for saving model artifacts\n",
    "# `sagemaker_session`: SageMaker session object created earlier\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m4.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=sess,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# Set hyperparameters for the XGBoost training job\n",
    "# `max_depth`: Maximum tree depth for base learners\n",
    "# `eta`: Step size shrinkage to prevent overfitting\n",
    "# `gamma`: Minimum loss reduction required for further partitioning\n",
    "# `min_child_weight`: Minimum sum of instance weight needed in a child node\n",
    "# `subsample`: Fraction of samples used per tree\n",
    "# `silent`: Verbosity (0 means silent mode)\n",
    "# `objective`: Learning objective (here, binary classification)\n",
    "# `num_round`: Number of boosting rounds\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    silent=0,\n",
    "    objective='binary:logistic',\n",
    "    num_round=100\n",
    ")\n",
    "\n",
    "# Launch the training job, passing in the S3 paths for training and validation datasets\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506dd14-8da1-441b-8103-6873cd57aef2",
   "metadata": {},
   "source": [
    "### Deploy Trained XGBoost Model as a Real-Time Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa738005-9d0f-4a05-a1f4-9f7bbae72510",
   "metadata": {},
   "source": [
    "This code deploys the trained XGBoost model to an Amazon SageMaker endpoint for real-time inference. The deploy() method sets up a fully managed endpoint, allowing the model to serve predictions via API requests. By specifying initial_instance_count and instance_type, you can control the scalability and resource allocation for handling inference requests. This deployment enables the model to be used for predictions in a production setting, supporting applications that require low-latency, real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fd7f65-f60a-412c-9d1a-39081acb9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: xgboost-2024-11-12-13-25-25-216\n",
      "INFO:sagemaker:Creating endpoint-config with name xgboost-2024-11-12-13-25-25-216\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-2024-11-12-13-25-25-216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained XGBoost model as an endpoint for real-time inference\n",
    "# `initial_instance_count`: Number of instances to serve predictions\n",
    "# `instance_type`: Type of instance to host the endpoint\n",
    "\n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc4c5d-cf10-4850-9679-1cdec14d0a0d",
   "metadata": {},
   "source": [
    "### Configure Serializer for Model Endpoint Input Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d144eb-44fa-4e18-b770-903e40d6c327",
   "metadata": {},
   "source": [
    "This code configures the input data format for requests sent to the deployed SageMaker endpoint. By setting the serializer to CSVSerializer, input data is converted to CSV format before it is passed to the endpoint for inference. This format aligns with the trained XGBoost model’s expectations, ensuring smooth data processing and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ada23f-0001-4530-ae87-fca8b8bbb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the serializer for the predictor to format input data as CSV\n",
    "# This serializer ensures that input data sent to the endpoint is correctly formatted as CSV, matching the model's expected input format\n",
    "\n",
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796a9735-a193-46ae-afb6-ef5b59df62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 08:06:30     498229 test_script_x.csv\n",
      "2024-11-03 08:06:30       8238 test_script_y.csv\n"
     ]
    }
   ],
   "source": [
    "# Use AWS CLI to list all files in the specified S3 directory for test data\n",
    "# `$test_path` contains the path to the S3 bucket and folder where test data files are stored\n",
    "\n",
    "!aws s3 ls $test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ab9af2-27c3-43a3-b2d9-fde5cd5cf6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping s3fs as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: fsspec 2023.6.0\n",
      "Uninstalling fsspec-2023.6.0:\n",
      "  Successfully uninstalled fsspec-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "# Use pip to uninstall the `s3fs` and `fsspec` packages\n",
    "# `-y` automatically confirms the uninstallation\n",
    "\n",
    "!pip uninstall -y s3fs fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ef957b-7e3d-4fdc-9a32-66a8531b75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs==2023.6.0\n",
      "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting fsspec==2023.6.0\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting aiobotocore~=2.5.0 (from s3fs==2023.6.0)\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from s3fs==2023.6.0) (3.9.5)\n",
      "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.0->s3fs==2023.6.0)\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.11/site-packages (from aiobotocore~=2.5.0->s3fs==2023.6.0) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore~=2.5.0->s3fs==2023.6.0) (0.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (1.15.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (2.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.26.19)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.16.0)\n",
      "Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.162\n",
      "    Uninstalling botocore-1.34.162:\n",
      "      Successfully uninstalled botocore-1.34.162\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 2.13.3\n",
      "    Uninstalling aiobotocore-2.13.3:\n",
      "      Successfully uninstalled aiobotocore-2.13.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "amazon-sagemaker-jupyter-scheduler 3.1.6 requires aiobotocore<3,>=2.9, but you have aiobotocore 2.5.4 which is incompatible.\n",
      "amazon-sagemaker-sql-editor 0.1.12 requires aiobotocore<3,>=2.7.0, but you have aiobotocore 2.5.4 which is incompatible.\n",
      "amazon-sagemaker-sql-editor 0.1.12 requires botocore<2,>=1.31.64, but you have botocore 1.31.17 which is incompatible.\n",
      "boto3 1.34.162 requires botocore<1.35.0,>=1.34.162, but you have botocore 1.31.17 which is incompatible.\n",
      "s3transfer 0.10.3 requires botocore<2.0a.0,>=1.33.2, but you have botocore 1.31.17 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.24 requires aiobotocore>=2.7.0, but you have aiobotocore 2.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.5.4 botocore-1.31.17 fsspec-2023.6.0 s3fs-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "# Use pip to install specific versions of the `s3fs` and `fsspec` packages\n",
    "# This ensures compatibility with the required version for the project\n",
    "\n",
    "!pip install s3fs==2023.6.0 fsspec==2023.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d356b43b-8f10-4a87-bbfb-edc387ef2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72546e-53e9-408a-8b37-b750007555f8",
   "metadata": {},
   "source": [
    "### Load Test Data for Inference: Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63433b8-58dd-434a-8a86-c9b08b1428ad",
   "metadata": {},
   "source": [
    "This code loads the test data for inference from two CSV files stored in the S3 test path. The test_script_x.csv file contains the feature data (X), and test_script_y.csv contains the actual labels (y) for the test dataset. By setting header=None, the code ensures that the files are read without assuming any header row in the CSV files, which is useful when the data doesn't include column headers. These dataframes will be used for evaluating the model or making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6084abf2-8a11-4315-b1bb-41361abefd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data for features (X) and labels (y) from CSV files stored in the S3 test path\n",
    "# `test_script_x.csv` contains the features, and `test_script_y.csv` contains the corresponding labels\n",
    "# `header=None` ensures the CSV files are loaded without assuming any header row\n",
    "\n",
    "test_data_x = pd.read_csv(os.path.join(test_path, 'test_script_x.csv'), header=None)\n",
    "test_data_y = pd.read_csv(os.path.join(test_path, 'test_script_y.csv'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d8fba-edef-4343-b241-629feba1034e",
   "metadata": {},
   "source": [
    "### Batch Prediction for Large Datasets Using SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf1cc0-4704-4088-9b85-a77fa4cc56b7",
   "metadata": {},
   "source": [
    "This function, predict, is designed to handle large datasets by splitting the input data into smaller batches, making it easier to process and predict without overwhelming system resources. The data (input features) is split into batches of a specified size (500 rows by default), and each batch is sent to the deployed SageMaker model endpoint (predictor) for inference. The predictions are collected in a string, which is later converted into a numerical array using np.fromstring. The result is the final array of predictions for the entire dataset. The function is invoked on test_data_x, which contains the feature data for making predictions using the trained XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842f9587-8413-4e88-997e-ebff1d8f1cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to make predictions on large datasets by splitting the data into smaller batches\n",
    "# `data`: The input data to predict (features)\n",
    "# `predictor`: The SageMaker model predictor for inference\n",
    "# `rows`: The number of rows per batch (default is 500)\n",
    "# The function splits the data into batches to avoid memory overload and sends each batch to the endpoint for prediction\n",
    "\n",
    "def predict(data, predictor, rows=500):\n",
    "    # Split the input data into smaller batches of the specified size\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    \n",
    "    # Initialize an empty string to store the concatenated predictions\n",
    "    predictions = ''\n",
    "    \n",
    "    # Loop over each batch and request predictions from the SageMaker endpoint\n",
    "    for array in split_array:\n",
    "        # Make predictions and append the results to the predictions string\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    # Convert the comma-separated string of predictions into a numpy array\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "# Call the `predict` function on the test data using the XGBoost predictor\n",
    "predictions = predict(test_data_x, xgb_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d089bf-c401-478d-9eaf-8a8739763ec4",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix for Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196c884-63f1-419e-b43a-fb7ed195e9e0",
   "metadata": {},
   "source": [
    "This code generates a confusion matrix using pd.crosstab, which compares the predicted values with the actual labels from the test set. The index parameter contains the actual values (test_data_y[0]), and the columns parameter contains the rounded predictions (np.round(predictions)) to map them to discrete classes. The resulting matrix shows how many instances were correctly or incorrectly classified, providing an overview of the model’s classification accuracy. The matrix is labeled with actuals for true labels and predictions for the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1b55ce9-73a1-4e2c-9b66-3e34507371ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3584</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            3584   51\n",
       "1             383  101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix to evaluate the model's performance by comparing actual vs predicted values\n",
    "# `test_data_y[0]`: Actual labels (ground truth) for the test set\n",
    "# `predictions`: Predicted values from the model (rounded to nearest integer for classification)\n",
    "# The result is a crosstab showing how well the predictions match the actual labels\n",
    "\n",
    "pd.crosstab(index=test_data_y[0], columns=np.round(predictions), \n",
    "            rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "503c0a6e-c7f1-482f-b7b6-0b8e74a72e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: xgboost-2024-11-12-13-25-25-216\n",
      "INFO:sagemaker:Deleting endpoint with name: xgboost-2024-11-12-13-25-25-216\n"
     ]
    }
   ],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745bf4c1-0c50-4d8b-9eea-597195b9f3f3",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9be1a8-1616-436d-93b0-bd7793ea629b",
   "metadata": {},
   "source": [
    "### Initialize Boto3 Clients for SageMaker and SageMaker Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b0a456-b6ae-4c89-a525-c17806b72a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4277d1-c5de-4d7a-9438-374a0d771a6e",
   "metadata": {},
   "source": [
    "### Retrieve Model Artifacts from the Trained XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eca2d7-ad5e-4ca3-9d49-51ba75cef401",
   "metadata": {},
   "source": [
    "This code retrieves the location of the model artifacts from the trained XGBoost model. xgb.model_data contains the S3 URI where the trained model is stored, including the model's weights, configurations, and other necessary components. These artifacts are essential for making predictions and for future use, such as model deployment or re-training. The path returned points to the location where the model was saved after training, allowing further interactions with the trained model in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81fe4f8b-8b63-4e27-86e3-eb0559dfe204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-607119565685/mlops/activity-3/output/xgboost-2024-11-11-13-53-34-644/output/model.tar.gz'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the model artifacts (e.g., model weights and configurations) of the trained XGBoost model\n",
    "# `xgb.model_data` contains the S3 path to the model artifacts generated during training\n",
    "\n",
    "model_artifacts = xgb.model_data\n",
    "\n",
    "# Display the path to the model artifacts stored in S3\n",
    "model_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106c740-51e2-4473-8715-7f4760f390e0",
   "metadata": {},
   "source": [
    "### Create and Register a Serverless Model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aacd72-d7ff-449b-a88a-70dd90191c2c",
   "metadata": {},
   "source": [
    "This code creates a new model in Amazon SageMaker by specifying the model name, container image, model artifacts (stored in S3), and any necessary environment variables. The create_model() function registers the model, allowing it to be used for inference. A unique model name is generated using the current timestamp to avoid conflicts with existing models. The model container is specified in \"SingleModel\" mode, meaning only one model is deployed per container. After the model is created, the response contains the ARN (Amazon Resource Name) of the model, which uniquely identifies it within the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6746452e-f880-4638-8ca6-0a5f6da17970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: xgboost-serverless2024-11-11-14-03-11\n",
      "Model Arn: arn:aws:sagemaker:us-east-1:607119565685:model/xgboost-serverless2024-11-11-14-03-11\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Generate a unique model name based on the current time to ensure uniqueness\n",
    "model_name = \"xgboost-serverless\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + model_name)\n",
    "\n",
    "# Define dummy environment variables for the container\n",
    "# These variables can be used within the container to configure logging levels or other settings\n",
    "byo_container_env_vars = {\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\", \"SOME_ENV_VAR\": \"myEnvVar\"}\n",
    "\n",
    "# Create the model in SageMaker\n",
    "# `ModelName`: The unique name for the model being created\n",
    "# `Containers`: A list containing the container definition for the model\n",
    "# `Image`: The URI of the container image for the model\n",
    "# `ModelDataUrl`: The S3 URI pointing to the model artifacts\n",
    "# `Environment`: Environment variables that will be set in the container\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[\n",
    "        {\n",
    "            \"Image\": container,  # The XGBoost container retrieved earlier\n",
    "            \"Mode\": \"SingleModel\",  # The model type for inference\n",
    "            \"ModelDataUrl\": model_artifacts,  # The S3 URI for the model artifacts\n",
    "            \"Environment\": byo_container_env_vars,  # Custom environment variables for the container\n",
    "        }\n",
    "    ],\n",
    "    ExecutionRoleArn=role,  # The IAM role to allow SageMaker to interact with AWS resources\n",
    ")\n",
    "\n",
    "# Print the ARN of the created model for reference\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d00d0-ba2d-4ffd-9f20-26549ad79652",
   "metadata": {},
   "source": [
    "### Create a Serverless Endpoint Configuration for the XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f95f8d-22b3-4c6f-8671-5e486344d32e",
   "metadata": {},
   "source": [
    "This code creates an endpoint configuration for a serverless deployment of the XGBoost model in Amazon SageMaker. The configuration includes a unique name generated from the current timestamp and specifies a production variant, which defines how the model will be deployed. The ServerlessConfig includes the memory size (MemorySizeInMB) and the maximum concurrency (MaxConcurrency) for the serverless endpoint, determining the available resources for handling inference requests. After the configuration is created, the ARN (Amazon Resource Name) of the endpoint configuration is printed for reference, which can later be used to deploy the model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "305aad6b-1330-4f7e-b349-c00e4a5f66ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Configuration Arn: arn:aws:sagemaker:us-east-1:607119565685:endpoint-config/mlops-serverless-epc2024-11-11-14-04-53\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Generate a unique endpoint configuration name based on the current time\n",
    "xgboost_epc_name = \"mlops-serverless-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Create an endpoint configuration for the XGBoost model\n",
    "# `EndpointConfigName`: The unique name for the endpoint configuration\n",
    "# `ProductionVariants`: Defines the production variants, including the model and serverless configuration\n",
    "\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=xgboost_epc_name,  # Unique name for the endpoint configuration\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"byoVariant\",  # Name of the production variant\n",
    "            \"ModelName\": model_name,  # The registered model name\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": 3072,  # The amount of memory for the serverless endpoint (in MB)\n",
    "                \"MaxConcurrency\": 1,     # Maximum number of concurrent invocations\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the ARN of the created endpoint configuration for reference\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a7f05-5301-4635-b9a3-ccb655f684e8",
   "metadata": {},
   "source": [
    "### Create a Serverless Endpoint for the XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2412f4-dc09-4964-aedb-8ce56184e268",
   "metadata": {},
   "source": [
    "This code creates a new serverless endpoint in Amazon SageMaker using the previously defined endpoint configuration. The endpoint name is generated uniquely by appending the current timestamp to avoid name collisions. The create_endpoint() function creates the actual endpoint, which is used to invoke the model for real-time predictions. The response contains the ARN (Amazon Resource Name) of the endpoint, which is then printed for reference. This endpoint will be used to deploy the XGBoost model and serve real-time predictions in a serverless fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00c69147-3403-44d8-9ba1-4cc36330a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:607119565685:endpoint/xgboost-serverless-ep2024-11-11-14-04-55\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Generate a unique endpoint name based on the current time to ensure uniqueness\n",
    "endpoint_name = \"xgboost-serverless-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Create an endpoint using the previously created endpoint configuration\n",
    "# `EndpointName`: The unique name for the endpoint\n",
    "# `EndpointConfigName`: The name of the endpoint configuration that defines how the model will be deployed\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,  # Unique name for the endpoint\n",
    "    EndpointConfigName=xgboost_epc_name,  # The endpoint configuration containing serverless setup\n",
    ")\n",
    "\n",
    "# Print the ARN of the created endpoint for reference\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e34d69-07e2-41ae-abb3-a1f8031ef5c7",
   "metadata": {},
   "source": [
    "### Monitor Endpoint Creation Status Until InService"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd12110-679b-498b-864d-2e30f852fcfc",
   "metadata": {},
   "source": [
    "This code monitors the creation status of a SageMaker endpoint until it transitions to the \"InService\" state, indicating that the endpoint is ready to serve predictions. It uses the describe_endpoint() function to query the current status of the endpoint. The loop checks the status every 15 seconds and prints the status until the endpoint reaches \"InService.\" Once the endpoint is ready, the final response with endpoint details is returned. This ensures that the endpoint is fully set up before proceeding with making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be5778b0-befd-4d99-bd53-9b7454151c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'xgboost-serverless-ep2024-11-11-14-04-55',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:607119565685:endpoint/xgboost-serverless-ep2024-11-11-14-04-55',\n",
       " 'EndpointConfigName': 'mlops-serverless-epc2024-11-11-14-04-53',\n",
       " 'ProductionVariants': [{'VariantName': 'byoVariant',\n",
       "   'DeployedImages': [{'SpecifiedImage': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
       "     'ResolvedImage': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost@sha256:0c8f830ac408e6dee08445fb60392e9c3f05f790a4b3c07ec22327c08f75bcbf',\n",
       "     'ResolutionTime': datetime.datetime(2024, 11, 11, 14, 4, 57, 153000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 0,\n",
       "   'CurrentServerlessConfig': {'MemorySizeInMB': 3072, 'MaxConcurrency': 1}}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2024, 11, 11, 14, 4, 56, 478000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 11, 11, 14, 6, 49, 810000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '0a7ecb07-ca8b-4bcc-b388-ab033f1b606d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0a7ecb07-ca8b-4bcc-b388-ab033f1b606d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '788',\n",
       "   'date': 'Mon, 11 Nov 2024 14:36:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Describe the endpoint to check its creation status\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Continuously check the endpoint status until it reaches \"InService\" (i.e., ready for inference)\n",
    "# The loop checks the status every 15 seconds\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])  # Print the current status of the endpoint\n",
    "    time.sleep(15)  # Wait for 15 seconds before checking again\n",
    "\n",
    "# Once the status is \"InService\", the endpoint is ready\n",
    "describe_endpoint_response  # Return the final response when the endpoint is in service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c701b-4a87-47c0-be18-469e1aaed246",
   "metadata": {},
   "source": [
    "### Invoke a SageMaker Endpoint for Real-Time Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5898288-92fc-4cc3-8001-cdb232e73910",
   "metadata": {},
   "source": [
    "This code demonstrates how to invoke a SageMaker endpoint for real-time inference. The payload is a CSV-formatted string, representing a single input sample for the model. The invoke_endpoint() function is used to send the payload to the endpoint, and the response contains the prediction result. The ContentType=\"text/csv\" specifies that the payload is in CSV format. The result is read from the response's body, decoded, and printed. This allows you to get real-time predictions from the deployed model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3be37f48-fb06-4bb6-bfc8-79ffa020932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07072833180427551\n"
     ]
    }
   ],
   "source": [
    "# Define a payload for the prediction request, which is a CSV-formatted string in byte format\n",
    "payload = b\"3., 999.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0., 0.,   0.,   0.,   0.,   0.,   1.,   0.,   1.,   0.,   0.,   1., 0.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   1., 0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0., 0.,   1.,   0.\"\n",
    "\n",
    "# Invoke the SageMaker endpoint for real-time inference using the runtime client\n",
    "# `EndpointName`: The name of the deployed model endpoint\n",
    "# `Body`: The data (payload) being sent to the endpoint for prediction\n",
    "# `ContentType`: The format of the data being sent, which is CSV in this case\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,  # Name of the endpoint being invoked\n",
    "    Body=payload,  # The input data for prediction\n",
    "    ContentType=\"text/csv\",  # Content type, as the data is in CSV format\n",
    ")\n",
    "\n",
    "# Read and decode the prediction result from the response and print it\n",
    "print(response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa36cca2-6d2a-4fb7-8893-df466589f3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd1e7a81a-15dc-4b18-8e8d-23c180173fa0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd1e7a81a-15dc-4b18-8e8d-23c180173fa0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Mon, 11 Nov 2024 14:40:27 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_model(ModelName=model_name)\n",
    "client.delete_endpoint_config(EndpointConfigName=xgboost_epc_name)\n",
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde19a6f-2546-49eb-a9a6-bae8bf169128",
   "metadata": {},
   "source": [
    "# Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d1afc-0880-4d8c-86ae-558771b86a80",
   "metadata": {},
   "source": [
    "### Setting Up Hyperparameter Ranges for SageMaker Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ded86d-382d-46d8-864a-95d7d3770627",
   "metadata": {},
   "source": [
    "This code configures hyperparameter ranges to optimize a model’s performance on Amazon SageMaker. Each range defines values for SageMaker’s Hyperparameter Tuner to explore, helping find the best hyperparameter combination.\n",
    "\n",
    "- **`ContinuousParameter(0, 1)`**: Defines a range for continuous hyperparameters, which SageMaker will test within the specified range (0 to 1).\n",
    "- **`IntegerParameter(1, 10)`**: Defines a range for integer hyperparameters, allowing SageMaker to explore integer values between 1 and 10.\n",
    "- **`CategoricalParameter(...)`**: (Not used here) Defines a range for categorical values if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30ba65c8-9629-4ea5-b110-0b2e5704fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Define the range of hyperparameters to tune\n",
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0, 1),               # Learning rate, from 0 to 1\n",
    "    'min_child_weight': ContinuousParameter(1, 10), # Minimum sum of weights for child nodes, from 1 to 10\n",
    "    'alpha': ContinuousParameter(0, 2),             # L1 regularization term, from 0 to 2\n",
    "    'max_depth': IntegerParameter(1, 10)            # Maximum depth of trees, from 1 to 10\n",
    "}\n",
    "\n",
    "# Set the objective metric for tuning\n",
    "objective_metric_name = 'validation:auc' # The metric to optimize during tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4ed8d-4dcf-4e7c-bfd5-11a6c57d7f18",
   "metadata": {},
   "source": [
    "### Initializing Hyperparameter Tuner for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9f2c4-2271-483b-9d82-39b793566be9",
   "metadata": {},
   "source": [
    "This code creates a `HyperparameterTuner` to optimize an XGBoost model on SageMaker. It sets:\n",
    "\n",
    "- **`xgb`**: The XGBoost estimator to tune.\n",
    "- **`objective_metric_name`**: The metric to optimize (`validation:auc`).\n",
    "- **`hyperparameter_ranges`**: Parameter ranges for tuning.\n",
    "- **`max_jobs=20`**: Total tuning jobs.\n",
    "- **`max_parallel_jobs=3`**: Concurrent jobs allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd46915-d895-4ef2-bdbb-100d2824d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hyperparameter Tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb,                         # XGBoost estimator\n",
    "    objective_metric_name,       # Metric to optimize (validation:auc)\n",
    "    hyperparameter_ranges,       # Hyperparameter ranges to explore\n",
    "    max_jobs=20,                 # Total tuning jobs\n",
    "    max_parallel_jobs=3          # Concurrent jobs allowed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f6066-a968-42e4-af3c-76c0b6711d8c",
   "metadata": {},
   "source": [
    "### Launching Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db400e-6bfd-40e4-8611-2f1e6823d403",
   "metadata": {},
   "source": [
    "This code initiates the tuning job for the XGBoost model using SageMaker. It specifies the S3 input paths for the training and validation datasets.\n",
    "\n",
    "- **`{'train': s3_input_train}`**: S3 path to training data.\n",
    "- **`{'validation': s3_input_validation}`**: S3 path to validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0caf065-e07a-4d1f-b240-2047cf868623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: xgboost-241112-1342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# Launch the hyperparameter tuning job\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a223cb8-ae94-44b8-8884-505ceba1e3bb",
   "metadata": {},
   "source": [
    "### Checking Hyperparameter Tuning Job Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403e22c-f4c0-457e-a28f-754b489d9cab",
   "metadata": {},
   "source": [
    "This code retrieves the current status of the latest hyperparameter tuning job for the SageMaker model.\n",
    "\n",
    "- **`tuner.latest_tuning_job.job_name`**: Fetches the name of the most recent tuning job.\n",
    "- **`HyperParameterTuningJobStatus`**: Indicates the job's current status (e.g., `InProgress`, `Completed`, or `Failed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83ed7ad9-6c46-4a33-b0d9-bfc890cdb151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Completed'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Check the status of the latest hyperparameter tuning job\n",
    "status = boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    ")['HyperParameterTuningJobStatus']\n",
    "\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54eefd0-b5bc-4dfc-830d-0dafe3dc65fb",
   "metadata": {},
   "source": [
    "### Retrieving the Best Training Job from Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc7cfc-7fd7-4686-a42f-db5f7a0646bb",
   "metadata": {},
   "source": [
    "This code returns the training job name with the best performance from the completed hyperparameter tuning jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a36a83e-a912-4ead-93b8-328ca8c230ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-241112-1342-015-d109cea7'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the name of the best-performing training job\n",
    "best_job_name = tuner.best_training_job()\n",
    "best_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceccd9d-5d5c-4fd1-bada-e396cfa11635",
   "metadata": {},
   "source": [
    "### Deploying the Best Model from Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54c163-641e-4eeb-93ca-035504092e01",
   "metadata": {},
   "source": [
    "This code deploys the best model from the hyperparameter tuning job to an Amazon SageMaker endpoint.\n",
    "\n",
    "- **`initial_instance_count=1`**: Specifies one instance for deploying the model.\n",
    "- **`instance_type='ml.m4.xlarge'`**: Defines the instance type for hosting the model (in this case, a `ml.m4.xlarge` instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22195c53-4325-4d05-9f90-3277822ef539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-11-12 13:50:08 Starting - Found matching resource for reuse\n",
      "2024-11-12 13:50:08 Downloading - Downloading the training image\n",
      "2024-11-12 13:50:08 Training - Training image download completed. Training in progress.\n",
      "2024-11-12 13:50:08 Uploading - Uploading generated training model\n",
      "2024-11-12 13:50:08 Completed - Resource reused by training job: xgboost-241112-1342-018-383eacd5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: xgboost-2024-11-12-13-54-23-060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name xgboost-241112-1342-015-d109cea7\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-241112-1342-015-d109cea7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "# Deploy the best model from the tuning job to a SageMaker endpoint\n",
    "tuner_predictor = tuner.deploy(\n",
    "    initial_instance_count=1,          # Number of instances to deploy\n",
    "    instance_type='ml.m4.xlarge'      # Type of instance for hosting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13bd0d-b15f-46f4-8613-d47908973c33",
   "metadata": {},
   "source": [
    "### Setting the Serializer for Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906f8bf-4d66-4f05-bddd-76bfd2441759",
   "metadata": {},
   "source": [
    "This code configures the input data format for requests sent to the deployed SageMaker endpoint. By setting the serializer to CSVSerializer, input data is converted to CSV format before it is passed to the endpoint for inference. This format aligns with the trained XGBoost model’s expectations, ensuring smooth data processing and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0481730d-33a8-46f5-91ee-f4e34e60f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the serializer to handle CSV input format for inference\n",
    "tuner_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9d140-90f1-49f2-83be-e8bfdbe73af0",
   "metadata": {},
   "source": [
    "### Load Test Data for Inference: Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1adfd-b40c-4e33-81a6-39deef03468f",
   "metadata": {},
   "source": [
    "This code loads the test data for inference from two CSV files stored in the S3 test path. The test_script_x.csv file contains the feature data (X), and test_script_y.csv contains the actual labels (y) for the test dataset. By setting header=None, the code ensures that the files are read without assuming any header row in the CSV files, which is useful when the data doesn't include column headers. These dataframes will be used for evaluating the model or making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3503d99f-5ad1-4861-8024-a62861a7da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = pd.read_csv(os.path.join(test_path, 'test_script_x.csv'),header=None)\n",
    "test_data_y = pd.read_csv(os.path.join(test_path, 'test_script_y.csv'),header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2d801-c489-4f6a-8b1a-5848f16ed350",
   "metadata": {},
   "source": [
    "### Making Predictions with the Deployed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d6d56-9cce-4c49-a224-5b9e21841234",
   "metadata": {},
   "source": [
    "This code sends the test data to the deployed model and retrieves predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146a80a4-bb1b-4229-b3ca-798b68b8166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the deployed model and convert the result to a NumPy array\n",
    "predictions = predict(test_data_x.to_numpy(), tuner_predictor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62ff6c-23ee-4fb2-97a0-ebaf35158763",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix for Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c5083-ff45-4600-96c3-70f6a746fab9",
   "metadata": {},
   "source": [
    "This code generates a confusion matrix using pd.crosstab, which compares the predicted values with the actual labels from the test set. The index parameter contains the actual values (test_data_y[0]), and the columns parameter contains the rounded predictions (np.round(predictions)) to map them to discrete classes. The resulting matrix shows how many instances were correctly or incorrectly classified, providing an overview of the model’s classification accuracy. The matrix is labeled with actuals for true labels and predictions for the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ee69c0d-ba15-4d31-9448-45bddb554f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3592</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            3592   43\n",
       "1             383  101"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix to evaluate the model's performance by comparing actual vs predicted values\n",
    "# `test_data_y[0]`: Actual labels (ground truth) for the test set\n",
    "# `predictions`: Predicted values from the model (rounded to nearest integer for classification)\n",
    "# The result is a crosstab showing how well the predictions match the actual labels\n",
    "\n",
    "pd.crosstab(index=test_data_y[0], columns=np.round(predictions), \n",
    "            rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1c9ac-0848-4625-b3b1-92c666fc9b2f",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75c38131-ab12-472f-9814-ba1988a61349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: xgboost-241112-1342-015-d109cea7\n",
      "INFO:sagemaker:Deleting endpoint with name: xgboost-241112-1342-015-d109cea7\n"
     ]
    }
   ],
   "source": [
    "tuner_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
