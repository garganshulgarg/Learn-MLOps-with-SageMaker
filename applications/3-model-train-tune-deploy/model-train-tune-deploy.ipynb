{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b71290e-32c6-4aa8-b9e7-726aafd404e9",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc5612-861f-4738-b19a-34eec0723309",
   "metadata": {},
   "source": [
    "### Initialize AWS SageMaker Environment and Define Data Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bca9c-a47f-4639-a573-2a4700f9322e",
   "metadata": {},
   "source": [
    "This code sets up the environment for working with Amazon SageMaker by importing necessary libraries and initializing key variables. It begins by establishing an AWS SageMaker session and obtaining the execution role, which grants permissions to interact with AWS resources. A default S3 bucket is defined to store data, and a prefix is used to organize activity-specific data paths within this bucket. Paths for training, validation, and testing data are specified, each stored in its respective folder within the S3 bucket, making it easier to access and manage data for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9159f8e-b23a-487d-b172-aaccccba8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for SageMaker session, AWS SDK, and data manipulation\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Obtain the SageMaker execution role for the notebook, which grants permissions to access AWS resources\n",
    "role = get_execution_role()\n",
    "\n",
    "# Set the default S3 bucket for SageMaker sessions; if none exists, SageMaker will create one\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "# Define a prefix for S3 paths to organize activity-related data within the bucket\n",
    "prefix = 'mlops/activity-3'\n",
    "\n",
    "# Initiate a SageMaker session, which is used to handle interactions with SageMaker APIs\n",
    "sess = Session()\n",
    "\n",
    "# Define S3 paths for train, validation, and test data, organized by prefix within the S3 bucket\n",
    "train_path = f\"s3://{bucket}/{prefix}/train/\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation/\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd370c-8cea-434d-a293-068d82de4a4e",
   "metadata": {},
   "source": [
    "### Retrieve Amazon SageMaker XGBoost Container Image URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbdc9e-b246-46bc-b95f-9756fa4db731",
   "metadata": {},
   "source": [
    "This code retrieves the Amazon SageMaker container image URI for the latest version of the XGBoost framework, ensuring compatibility with the current AWS region. The container URI is essential for launching and managing an XGBoost training job on SageMaker, as it provides the runtime environment pre-configured with the necessary libraries and dependencies. This region-specific approach ensures that the container image URI corresponds to resources available in the current region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c9d6572-1cf2-4bf7-9a3f-0a71304a86cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the Amazon SageMaker container URI for the latest version of the XGBoost framework\n",
    "# This is region-specific, so it uses the current AWS session's region\n",
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, \n",
    "                                          framework='xgboost', \n",
    "                                          version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56338f8-36b8-43d8-bbe5-1563fd485d76",
   "metadata": {},
   "source": [
    "### Define S3 Input Data Locations for SageMaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf052d-0e01-47b0-bcb5-9aecec27de8c",
   "metadata": {},
   "source": [
    "This code defines the input data locations for both training and validation datasets, stored in Amazon S3, to be used in an Amazon SageMaker training job. Using the TrainingInput class, SageMaker recognizes these data inputs and allows seamless access during training. The content_type parameter is set to 'csv' to indicate that the input files are in CSV format, which is essential for compatibility with SageMaker’s data ingestion pipeline. By organizing data in this way, training and validation data can be easily referenced and managed in subsequent stages of the machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4d85016-754e-4c64-889a-b4f9799d7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 input locations for training and validation data\n",
    "# The `TrainingInput` class indicates that these data inputs will be used in a training job\n",
    "# `content_type='csv'` specifies that the data is in CSV format\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f's3://{bucket}/{prefix}/train',\n",
    "    content_type='csv'\n",
    ")\n",
    "\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f's3://{bucket}/{prefix}/validation/',\n",
    "    content_type='csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4b39c-f212-42e4-b1e6-6b3f4003c10d",
   "metadata": {},
   "source": [
    "### Set Up and Train an XGBoost Model on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2be42-ea9d-43b1-abbf-e060910fef58",
   "metadata": {},
   "source": [
    "This code configures and initiates the training of an XGBoost model on Amazon SageMaker. Using the Estimator class, it specifies key parameters such as the instance type, IAM role, and the S3 location where the trained model will be stored. The model's hyperparameters, including max_depth, eta, gamma, min_child_weight, and subsample, are set to optimize the XGBoost algorithm for binary classification. Finally, the .fit() method starts the training job using the designated training and validation datasets stored in S3, enabling SageMaker to process the data and train the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6eb5cb34-5635-4606-9cbd-d049acbbe39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgboost-2024-11-11-15-01-01-311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: xgboost2024-11-11-15-01-01\n",
      "2024-11-11 15:01:04 Starting - Starting the training job......\n",
      "2024-11-11 15:01:49 Starting - Preparing the instances for training......\n",
      "2024-11-11 15:02:53 Downloading - Downloading input data...\n",
      "2024-11-11 15:03:23 Downloading - Downloading the training image......\n",
      "2024-11-11 15:04:29 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2024-11-11:15:04:39:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2024-11-11:15:04:39:INFO] File size need to be processed in the node: 4.35mb. Available memory size in the node: 8458.49mb\u001b[0m\n",
      "\u001b[34m[2024-11-11:15:04:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[15:04:39] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[15:04:39] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2024-11-11:15:04:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[15:04:39] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[15:04:40] 8238x59 matrix with 486042 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.101384#011validation-error:0.102209\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.101037#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.100205#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.100309#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.101141#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.100447#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.100864#011validation-error:0.100024\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.10076#011validation-error:0.100267\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.100482#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.100621#011validation-error:0.100267\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.100309#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.100066#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.099997#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.099997#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.099754#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.099719#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.099754#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.099684#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.099823#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.099858#011validation-error:0.101602\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.099927#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.099546#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.099268#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09906#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.098887#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.098921#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.099164#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.098991#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.099025#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.098783#011validation-error:0.100024\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.098644#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.098609#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 28 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.098262#011validation-error:0.099903\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.098297#011validation-error:0.099781\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.098332#011validation-error:0.099903\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.098332#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.098019#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.098019#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.098019#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.098054#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.097985#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.098193#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.098089#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.098158#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.098262#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.098297#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.098193#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 20 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.098124#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 22 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.09795#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.098019#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 34 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.098089#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.098124#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.097777#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.097569#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.097638#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.097777#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.097569#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.097673#011validation-error:0.100388\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.097569#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.097465#011validation-error:0.10051\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.097395#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.097395#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.097256#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.097256#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.097256#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.097187#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.09736#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.097291#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.09736#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 34 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.097326#011validation-error:0.101481\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.097291#011validation-error:0.101602\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.097152#011validation-error:0.10136\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.097083#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.097152#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.097014#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.097083#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 28 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.097048#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.097118#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.09691#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 24 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.096771#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.096632#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.096806#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 30 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.096667#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 38 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.096632#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.096632#011validation-error:0.100631\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.096632#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.096632#011validation-error:0.101117\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.096736#011validation-error:0.101238\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.096701#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.096597#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.096528#011validation-error:0.100753\u001b[0m\n",
      "\u001b[34m[15:04:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.096493#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.096493#011validation-error:0.100874\u001b[0m\n",
      "\u001b[34m[15:04:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.096493#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.096424#011validation-error:0.100995\u001b[0m\n",
      "\u001b[34m[15:04:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.096355#011validation-error:0.100995\u001b[0m\n",
      "\n",
      "2024-11-11 15:05:03 Uploading - Uploading generated training model\n",
      "2024-11-11 15:05:03 Completed - Training job completed\n",
      "Training seconds: 130\n",
      "Billable seconds: 130\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "# Initialize a new SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "# Generate a unique model name based on the current time to ensure uniqueness\n",
    "model_name = \"xgboost\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + model_name)\n",
    "\n",
    "# Define an XGBoost estimator using the SageMaker Estimator API\n",
    "# `container`: URI of the XGBoost container image (retrieved earlier)\n",
    "# `role`: IAM role with permissions for SageMaker to access AWS resources\n",
    "# `instance_count`: Number of compute instances to use\n",
    "# `instance_type`: Type of SageMaker instance for training\n",
    "# `output_path`: S3 location for saving model artifacts\n",
    "# `sagemaker_session`: SageMaker session object created earlier\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m4.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=sess,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# Set hyperparameters for the XGBoost training job\n",
    "# `max_depth`: Maximum tree depth for base learners\n",
    "# `eta`: Step size shrinkage to prevent overfitting\n",
    "# `gamma`: Minimum loss reduction required for further partitioning\n",
    "# `min_child_weight`: Minimum sum of instance weight needed in a child node\n",
    "# `subsample`: Fraction of samples used per tree\n",
    "# `silent`: Verbosity (0 means silent mode)\n",
    "# `objective`: Learning objective (here, binary classification)\n",
    "# `num_round`: Number of boosting rounds\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    silent=0,\n",
    "    objective='binary:logistic',\n",
    "    num_round=100\n",
    ")\n",
    "\n",
    "# Launch the training job, passing in the S3 paths for training and validation datasets\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506dd14-8da1-441b-8103-6873cd57aef2",
   "metadata": {},
   "source": [
    "### Deploy Trained XGBoost Model as a Real-Time Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa738005-9d0f-4a05-a1f4-9f7bbae72510",
   "metadata": {},
   "source": [
    "This code deploys the trained XGBoost model to an Amazon SageMaker endpoint for real-time inference. The deploy() method sets up a fully managed endpoint, allowing the model to serve predictions via API requests. By specifying initial_instance_count and instance_type, you can control the scalability and resource allocation for handling inference requests. This deployment enables the model to be used for predictions in a production setting, supporting applications that require low-latency, real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6fd7f65-f60a-412c-9d1a-39081acb9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: xgboost-2024-11-11-15-05-18-514\n",
      "INFO:sagemaker:Creating endpoint-config with name xgboost-2024-11-11-15-05-18-514\n",
      "INFO:sagemaker:Creating endpoint with name xgboost-2024-11-11-15-05-18-514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained XGBoost model as an endpoint for real-time inference\n",
    "# `initial_instance_count`: Number of instances to serve predictions\n",
    "# `instance_type`: Type of instance to host the endpoint\n",
    "\n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc4c5d-cf10-4850-9679-1cdec14d0a0d",
   "metadata": {},
   "source": [
    "### Configure Serializer for Model Endpoint Input Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d144eb-44fa-4e18-b770-903e40d6c327",
   "metadata": {},
   "source": [
    "This code configures the input data format for requests sent to the deployed SageMaker endpoint. By setting the serializer to CSVSerializer, input data is converted to CSV format before it is passed to the endpoint for inference. This format aligns with the trained XGBoost model’s expectations, ensuring smooth data processing and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ada23f-0001-4530-ae87-fca8b8bbb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the serializer for the predictor to format input data as CSV\n",
    "# This serializer ensures that input data sent to the endpoint is correctly formatted as CSV, matching the model's expected input format\n",
    "\n",
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "796a9735-a193-46ae-afb6-ef5b59df62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 08:06:30     498229 test_script_x.csv\n",
      "2024-11-03 08:06:30       8238 test_script_y.csv\n"
     ]
    }
   ],
   "source": [
    "# Use AWS CLI to list all files in the specified S3 directory for test data\n",
    "# `$test_path` contains the path to the S3 bucket and folder where test data files are stored\n",
    "\n",
    "!aws s3 ls $test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00ab9af2-27c3-43a3-b2d9-fde5cd5cf6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: s3fs 2023.6.0\n",
      "Uninstalling s3fs-2023.6.0:\n",
      "  Successfully uninstalled s3fs-2023.6.0\n",
      "Found existing installation: fsspec 2023.6.0\n",
      "Uninstalling fsspec-2023.6.0:\n",
      "  Successfully uninstalled fsspec-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "# Use pip to uninstall the `s3fs` and `fsspec` packages\n",
    "# `-y` automatically confirms the uninstallation\n",
    "\n",
    "!pip uninstall -y s3fs fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9ef957b-7e3d-4fdc-9a32-66a8531b75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs==2023.6.0\n",
      "  Using cached s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting fsspec==2023.6.0\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiobotocore~=2.5.0 in /opt/conda/lib/python3.11/site-packages (from s3fs==2023.6.0) (2.5.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from s3fs==2023.6.0) (3.9.5)\n",
      "Requirement already satisfied: botocore<1.31.18,>=1.31.17 in /opt/conda/lib/python3.11/site-packages (from aiobotocore~=2.5.0->s3fs==2023.6.0) (1.31.17)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.11/site-packages (from aiobotocore~=2.5.0->s3fs==2023.6.0) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore~=2.5.0->s3fs==2023.6.0) (0.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (1.15.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (2.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.26.19)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.16.0)\n",
      "Using cached s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
      "Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: fsspec, s3fs\n",
      "Successfully installed fsspec-2023.6.0 s3fs-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "# Use pip to install specific versions of the `s3fs` and `fsspec` packages\n",
    "# This ensures compatibility with the required version for the project\n",
    "\n",
    "!pip install s3fs==2023.6.0 fsspec==2023.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d356b43b-8f10-4a87-bbfb-edc387ef2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72546e-53e9-408a-8b37-b750007555f8",
   "metadata": {},
   "source": [
    "### Load Test Data for Inference: Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63433b8-58dd-434a-8a86-c9b08b1428ad",
   "metadata": {},
   "source": [
    "This code loads the test data for inference from two CSV files stored in the S3 test path. The test_script_x.csv file contains the feature data (X), and test_script_y.csv contains the actual labels (y) for the test dataset. By setting header=None, the code ensures that the files are read without assuming any header row in the CSV files, which is useful when the data doesn't include column headers. These dataframes will be used for evaluating the model or making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6084abf2-8a11-4315-b1bb-41361abefd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data for features (X) and labels (y) from CSV files stored in the S3 test path\n",
    "# `test_script_x.csv` contains the features, and `test_script_y.csv` contains the corresponding labels\n",
    "# `header=None` ensures the CSV files are loaded without assuming any header row\n",
    "\n",
    "test_data_x = pd.read_csv(os.path.join(test_path, 'test_script_x.csv'), header=None)\n",
    "test_data_y = pd.read_csv(os.path.join(test_path, 'test_script_y.csv'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d8fba-edef-4343-b241-629feba1034e",
   "metadata": {},
   "source": [
    "### Batch Prediction for Large Datasets Using SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf1cc0-4704-4088-9b85-a77fa4cc56b7",
   "metadata": {},
   "source": [
    "This function, predict, is designed to handle large datasets by splitting the input data into smaller batches, making it easier to process and predict without overwhelming system resources. The data (input features) is split into batches of a specified size (500 rows by default), and each batch is sent to the deployed SageMaker model endpoint (predictor) for inference. The predictions are collected in a string, which is later converted into a numerical array using np.fromstring. The result is the final array of predictions for the entire dataset. The function is invoked on test_data_x, which contains the feature data for making predictions using the trained XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "842f9587-8413-4e88-997e-ebff1d8f1cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to make predictions on large datasets by splitting the data into smaller batches\n",
    "# `data`: The input data to predict (features)\n",
    "# `predictor`: The SageMaker model predictor for inference\n",
    "# `rows`: The number of rows per batch (default is 500)\n",
    "# The function splits the data into batches to avoid memory overload and sends each batch to the endpoint for prediction\n",
    "\n",
    "def predict(data, predictor, rows=500):\n",
    "    # Split the input data into smaller batches of the specified size\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    \n",
    "    # Initialize an empty string to store the concatenated predictions\n",
    "    predictions = ''\n",
    "    \n",
    "    # Loop over each batch and request predictions from the SageMaker endpoint\n",
    "    for array in split_array:\n",
    "        # Make predictions and append the results to the predictions string\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    # Convert the comma-separated string of predictions into a numpy array\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "# Call the `predict` function on the test data using the XGBoost predictor\n",
    "predictions = predict(test_data_x, xgb_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d089bf-c401-478d-9eaf-8a8739763ec4",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix for Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196c884-63f1-419e-b43a-fb7ed195e9e0",
   "metadata": {},
   "source": [
    "This code generates a confusion matrix using pd.crosstab, which compares the predicted values with the actual labels from the test set. The index parameter contains the actual values (test_data_y[0]), and the columns parameter contains the rounded predictions (np.round(predictions)) to map them to discrete classes. The resulting matrix shows how many instances were correctly or incorrectly classified, providing an overview of the model’s classification accuracy. The matrix is labeled with actuals for true labels and predictions for the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1b55ce9-73a1-4e2c-9b66-3e34507371ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3584</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            3584   51\n",
       "1             383  101"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix to evaluate the model's performance by comparing actual vs predicted values\n",
    "# `test_data_y[0]`: Actual labels (ground truth) for the test set\n",
    "# `predictions`: Predicted values from the model (rounded to nearest integer for classification)\n",
    "# The result is a crosstab showing how well the predictions match the actual labels\n",
    "\n",
    "pd.crosstab(index=test_data_y[0], columns=np.round(predictions), \n",
    "            rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "503c0a6e-c7f1-482f-b7b6-0b8e74a72e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: xgboost-2024-11-11-15-05-18-514\n",
      "INFO:sagemaker:Deleting endpoint with name: xgboost-2024-11-11-15-05-18-514\n"
     ]
    }
   ],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1face933-a7c6-4b05-87d9-54afd9040173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "745bf4c1-0c50-4d8b-9eea-597195b9f3f3",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9be1a8-1616-436d-93b0-bd7793ea629b",
   "metadata": {},
   "source": [
    "### Initialize Boto3 Clients for SageMaker and SageMaker Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b0a456-b6ae-4c89-a525-c17806b72a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4277d1-c5de-4d7a-9438-374a0d771a6e",
   "metadata": {},
   "source": [
    "### Retrieve Model Artifacts from the Trained XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eca2d7-ad5e-4ca3-9d49-51ba75cef401",
   "metadata": {},
   "source": [
    "This code retrieves the location of the model artifacts from the trained XGBoost model. xgb.model_data contains the S3 URI where the trained model is stored, including the model's weights, configurations, and other necessary components. These artifacts are essential for making predictions and for future use, such as model deployment or re-training. The path returned points to the location where the model was saved after training, allowing further interactions with the trained model in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81fe4f8b-8b63-4e27-86e3-eb0559dfe204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-607119565685/mlops/activity-3/output/xgboost-2024-11-11-13-53-34-644/output/model.tar.gz'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the model artifacts (e.g., model weights and configurations) of the trained XGBoost model\n",
    "# `xgb.model_data` contains the S3 path to the model artifacts generated during training\n",
    "\n",
    "model_artifacts = xgb.model_data\n",
    "\n",
    "# Display the path to the model artifacts stored in S3\n",
    "model_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106c740-51e2-4473-8715-7f4760f390e0",
   "metadata": {},
   "source": [
    "### Create and Register a Serverless Model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aacd72-d7ff-449b-a88a-70dd90191c2c",
   "metadata": {},
   "source": [
    "This code creates a new model in Amazon SageMaker by specifying the model name, container image, model artifacts (stored in S3), and any necessary environment variables. The create_model() function registers the model, allowing it to be used for inference. A unique model name is generated using the current timestamp to avoid conflicts with existing models. The model container is specified in \"SingleModel\" mode, meaning only one model is deployed per container. After the model is created, the response contains the ARN (Amazon Resource Name) of the model, which uniquely identifies it within the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6746452e-f880-4638-8ca6-0a5f6da17970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: xgboost-serverless2024-11-11-14-03-11\n",
      "Model Arn: arn:aws:sagemaker:us-east-1:607119565685:model/xgboost-serverless2024-11-11-14-03-11\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Generate a unique model name based on the current time to ensure uniqueness\n",
    "model_name = \"xgboost-serverless\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + model_name)\n",
    "\n",
    "# Define dummy environment variables for the container\n",
    "# These variables can be used within the container to configure logging levels or other settings\n",
    "byo_container_env_vars = {\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\", \"SOME_ENV_VAR\": \"myEnvVar\"}\n",
    "\n",
    "# Create the model in SageMaker\n",
    "# `ModelName`: The unique name for the model being created\n",
    "# `Containers`: A list containing the container definition for the model\n",
    "# `Image`: The URI of the container image for the model\n",
    "# `ModelDataUrl`: The S3 URI pointing to the model artifacts\n",
    "# `Environment`: Environment variables that will be set in the container\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[\n",
    "        {\n",
    "            \"Image\": container,  # The XGBoost container retrieved earlier\n",
    "            \"Mode\": \"SingleModel\",  # The model type for inference\n",
    "            \"ModelDataUrl\": model_artifacts,  # The S3 URI for the model artifacts\n",
    "            \"Environment\": byo_container_env_vars,  # Custom environment variables for the container\n",
    "        }\n",
    "    ],\n",
    "    ExecutionRoleArn=role,  # The IAM role to allow SageMaker to interact with AWS resources\n",
    ")\n",
    "\n",
    "# Print the ARN of the created model for reference\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d00d0-ba2d-4ffd-9f20-26549ad79652",
   "metadata": {},
   "source": [
    "### Create a Serverless Endpoint Configuration for the XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f95f8d-22b3-4c6f-8671-5e486344d32e",
   "metadata": {},
   "source": [
    "This code creates an endpoint configuration for a serverless deployment of the XGBoost model in Amazon SageMaker. The configuration includes a unique name generated from the current timestamp and specifies a production variant, which defines how the model will be deployed. The ServerlessConfig includes the memory size (MemorySizeInMB) and the maximum concurrency (MaxConcurrency) for the serverless endpoint, determining the available resources for handling inference requests. After the configuration is created, the ARN (Amazon Resource Name) of the endpoint configuration is printed for reference, which can later be used to deploy the model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "305aad6b-1330-4f7e-b349-c00e4a5f66ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Configuration Arn: arn:aws:sagemaker:us-east-1:607119565685:endpoint-config/mlops-serverless-epc2024-11-11-14-04-53\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Generate a unique endpoint configuration name based on the current time\n",
    "xgboost_epc_name = \"mlops-serverless-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Create an endpoint configuration for the XGBoost model\n",
    "# `EndpointConfigName`: The unique name for the endpoint configuration\n",
    "# `ProductionVariants`: Defines the production variants, including the model and serverless configuration\n",
    "\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=xgboost_epc_name,  # Unique name for the endpoint configuration\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"byoVariant\",  # Name of the production variant\n",
    "            \"ModelName\": model_name,  # The registered model name\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": 3072,  # The amount of memory for the serverless endpoint (in MB)\n",
    "                \"MaxConcurrency\": 1,     # Maximum number of concurrent invocations\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the ARN of the created endpoint configuration for reference\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a7f05-5301-4635-b9a3-ccb655f684e8",
   "metadata": {},
   "source": [
    "### Create a Serverless Endpoint for the XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2412f4-dc09-4964-aedb-8ce56184e268",
   "metadata": {},
   "source": [
    "This code creates a new serverless endpoint in Amazon SageMaker using the previously defined endpoint configuration. The endpoint name is generated uniquely by appending the current timestamp to avoid name collisions. The create_endpoint() function creates the actual endpoint, which is used to invoke the model for real-time predictions. The response contains the ARN (Amazon Resource Name) of the endpoint, which is then printed for reference. This endpoint will be used to deploy the XGBoost model and serve real-time predictions in a serverless fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00c69147-3403-44d8-9ba1-4cc36330a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:607119565685:endpoint/xgboost-serverless-ep2024-11-11-14-04-55\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Generate a unique endpoint name based on the current time to ensure uniqueness\n",
    "endpoint_name = \"xgboost-serverless-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Create an endpoint using the previously created endpoint configuration\n",
    "# `EndpointName`: The unique name for the endpoint\n",
    "# `EndpointConfigName`: The name of the endpoint configuration that defines how the model will be deployed\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,  # Unique name for the endpoint\n",
    "    EndpointConfigName=xgboost_epc_name,  # The endpoint configuration containing serverless setup\n",
    ")\n",
    "\n",
    "# Print the ARN of the created endpoint for reference\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e34d69-07e2-41ae-abb3-a1f8031ef5c7",
   "metadata": {},
   "source": [
    "### Monitor Endpoint Creation Status Until InService"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd12110-679b-498b-864d-2e30f852fcfc",
   "metadata": {},
   "source": [
    "This code monitors the creation status of a SageMaker endpoint until it transitions to the \"InService\" state, indicating that the endpoint is ready to serve predictions. It uses the describe_endpoint() function to query the current status of the endpoint. The loop checks the status every 15 seconds and prints the status until the endpoint reaches \"InService.\" Once the endpoint is ready, the final response with endpoint details is returned. This ensures that the endpoint is fully set up before proceeding with making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be5778b0-befd-4d99-bd53-9b7454151c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'xgboost-serverless-ep2024-11-11-14-04-55',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:607119565685:endpoint/xgboost-serverless-ep2024-11-11-14-04-55',\n",
       " 'EndpointConfigName': 'mlops-serverless-epc2024-11-11-14-04-53',\n",
       " 'ProductionVariants': [{'VariantName': 'byoVariant',\n",
       "   'DeployedImages': [{'SpecifiedImage': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
       "     'ResolvedImage': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost@sha256:0c8f830ac408e6dee08445fb60392e9c3f05f790a4b3c07ec22327c08f75bcbf',\n",
       "     'ResolutionTime': datetime.datetime(2024, 11, 11, 14, 4, 57, 153000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 0,\n",
       "   'CurrentServerlessConfig': {'MemorySizeInMB': 3072, 'MaxConcurrency': 1}}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2024, 11, 11, 14, 4, 56, 478000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 11, 11, 14, 6, 49, 810000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '0a7ecb07-ca8b-4bcc-b388-ab033f1b606d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0a7ecb07-ca8b-4bcc-b388-ab033f1b606d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '788',\n",
       "   'date': 'Mon, 11 Nov 2024 14:36:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Describe the endpoint to check its creation status\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Continuously check the endpoint status until it reaches \"InService\" (i.e., ready for inference)\n",
    "# The loop checks the status every 15 seconds\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])  # Print the current status of the endpoint\n",
    "    time.sleep(15)  # Wait for 15 seconds before checking again\n",
    "\n",
    "# Once the status is \"InService\", the endpoint is ready\n",
    "describe_endpoint_response  # Return the final response when the endpoint is in service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c701b-4a87-47c0-be18-469e1aaed246",
   "metadata": {},
   "source": [
    "### Invoke a SageMaker Endpoint for Real-Time Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5898288-92fc-4cc3-8001-cdb232e73910",
   "metadata": {},
   "source": [
    "This code demonstrates how to invoke a SageMaker endpoint for real-time inference. The payload is a CSV-formatted string, representing a single input sample for the model. The invoke_endpoint() function is used to send the payload to the endpoint, and the response contains the prediction result. The ContentType=\"text/csv\" specifies that the payload is in CSV format. The result is read from the response's body, decoded, and printed. This allows you to get real-time predictions from the deployed model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3be37f48-fb06-4bb6-bfc8-79ffa020932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07072833180427551\n"
     ]
    }
   ],
   "source": [
    "# Define a payload for the prediction request, which is a CSV-formatted string in byte format\n",
    "payload = b\"3., 999.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0., 0.,   0.,   0.,   0.,   0.,   1.,   0.,   1.,   0.,   0.,   1., 0.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   1., 0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0., 0.,   1.,   0.\"\n",
    "\n",
    "# Invoke the SageMaker endpoint for real-time inference using the runtime client\n",
    "# `EndpointName`: The name of the deployed model endpoint\n",
    "# `Body`: The data (payload) being sent to the endpoint for prediction\n",
    "# `ContentType`: The format of the data being sent, which is CSV in this case\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,  # Name of the endpoint being invoked\n",
    "    Body=payload,  # The input data for prediction\n",
    "    ContentType=\"text/csv\",  # Content type, as the data is in CSV format\n",
    ")\n",
    "\n",
    "# Read and decode the prediction result from the response and print it\n",
    "print(response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa36cca2-6d2a-4fb7-8893-df466589f3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd1e7a81a-15dc-4b18-8e8d-23c180173fa0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd1e7a81a-15dc-4b18-8e8d-23c180173fa0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Mon, 11 Nov 2024 14:40:27 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_model(ModelName=model_name)\n",
    "client.delete_endpoint_config(EndpointConfigName=xgboost_epc_name)\n",
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c3880-208c-470a-8e6e-05fdf10f1f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
